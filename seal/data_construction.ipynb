{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998c244b-7069-4b97-8387-11c6e27f2f48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest\n",
    "import json\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43864ed6-a960-4bae-a1e0-a3f1f9354ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Define math problems for testing\n",
    "file_path = \"/home/xhl/eval/my_eval/data/math/train.jsonl\"\n",
    "problems = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        problems.append(item[\"problem\"])\n",
    "\n",
    "# Create prompt texts from problems\n",
    "texts = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + problem + \"\\nAssistant: <think>\" for problem in problems]\n",
    "\n",
    "# Generate answers using the LLM\n",
    "answers = llm.generate(\n",
    "    texts[:1000],\n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=8192,\n",
    "        skip_special_tokens=False,\n",
    "    ),\n",
    ")\n",
    "answers = [answer.outputs[0].text for answer in answers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c28f54-d87f-4180-a8ba-9b0936d0102c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 假设 texts 和 answers 已经生成\n",
    "# 保存到文件\n",
    "save_path = \"results.json\"\n",
    "with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"texts\": texts[:1000], \"answers\": answers}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"已保存到 {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cec02f-6f47-4696-a4f5-b0b5db5ce425",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "    texts = data[\"texts\"]\n",
    "    answers = data[\"answers\"]\n",
    "\n",
    "print(\"加载成功，数量：\", len(texts), len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c364a9-0d14-4fe1-af4b-ffac0192e8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Create QA pairs by combining prompts and answers\n",
    "qa_pairs = [texts[i] + answers[i] for i in range(len(texts))]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\")\n",
    "\n",
    "# The newline token suffix in tokenizer vocabulary\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Process each QA pair to find newline positions\n",
    "all_tokens_list = []\n",
    "all_newline_positions = []\n",
    "for qa in qa_pairs:\n",
    "    # Tokenize the QA pair\n",
    "    tokens = tokenizer.tokenize(qa, add_special_tokens=True)\n",
    "    all_tokens_list.append(tokens)\n",
    "    \n",
    "    # Find all positions of \"ĊĊ\" in the tokens\n",
    "    # These represent potential paragraph breaks in the text\n",
    "    positions = [\n",
    "        i for i, token in enumerate(tokens) \n",
    "        if isinstance(token, str) and token.endswith(target_suffix)\n",
    "    ]\n",
    "    all_newline_positions.append(positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa65041-c059-4fd6-8405-309d59649711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define keyword sets for classifying reasoning segments\n",
    "TRANSITION_KEYWORDS = [\n",
    "    'alternatively', 'think differently', 'another way', 'another approach',\n",
    "    'another method', 'another solution', 'another strategy', 'another technique'\n",
    "]\n",
    "\n",
    "REFLECTION_KEYWORDS = [\n",
    "    'wait', 'verify', 'make sure', 'hold on', 'think again', \"'s correct\",\n",
    "    \"'s incorrect\", 'let me check', 'seems right'\n",
    "]\n",
    "\n",
    "def classify_segment(text_segment):\n",
    "    \"\"\"\n",
    "    Classify text segments based on keyword matches.\n",
    "    \n",
    "    Args:\n",
    "        text_segment: String of text to classify\n",
    "        \n",
    "    Returns:\n",
    "        String category: \"Transition\", \"Reflection\", or \"Execution\"\n",
    "    \"\"\"\n",
    "    lower_text = text_segment.lower()\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in TRANSITION_KEYWORDS):\n",
    "        return \"Transition\"\n",
    "    \n",
    "    if any(keyword in lower_text for keyword in REFLECTION_KEYWORDS):\n",
    "        return \"Reflection\"\n",
    "    \n",
    "    # Default category is \"Execution\" (not \"Other\")\n",
    "    return \"Execution\"\n",
    "\n",
    "# Perform classification on all QA pairs\n",
    "all_classifications = []\n",
    "\n",
    "for i, positions in enumerate(all_newline_positions):\n",
    "    tokens = all_tokens_list[i]\n",
    "    classifications_for_qa = []\n",
    "\n",
    "    # Skip if no paragraph breaks were found\n",
    "    if not positions:\n",
    "        all_classifications.append(classifications_for_qa)\n",
    "        continue\n",
    "\n",
    "    # Classify each paragraph segment\n",
    "    for j, pos in enumerate(positions):\n",
    "        # Define segment boundaries\n",
    "        start_slice = pos + 1\n",
    "        end_slice = positions[j+1] if j + 1 < len(positions) else len(tokens)\n",
    "\n",
    "        # Extract and decode the text segment\n",
    "        token_slice = tokens[start_slice:end_slice]\n",
    "        text_segment = tokenizer.decode(\n",
    "            tokenizer.convert_tokens_to_ids(token_slice), \n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Classify the segment\n",
    "        category = classify_segment(text_segment)\n",
    "\n",
    "        # Store the classification result\n",
    "        classifications_for_qa.append({\n",
    "            \"position_in_tokens\": pos,\n",
    "            \"category\": category,\n",
    "        })\n",
    "\n",
    "    all_classifications.append(classifications_for_qa)\n",
    "\n",
    "# Print summary of classification results\n",
    "print(\"--- Summary of classification results for all samples ---\")\n",
    "\n",
    "for i, qa_results in enumerate(all_classifications):\n",
    "    print(f\"\\n--- Analysis of QA Pair {i+1} ---\")\n",
    "\n",
    "    # Group token positions by category\n",
    "    summary = {\n",
    "        \"Transition\": [],\n",
    "        \"Reflection\": [],\n",
    "        \"Execution\": []\n",
    "    }\n",
    "\n",
    "    # Collect positions for each category\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "        if category in summary:\n",
    "            summary[category].append(position)\n",
    "\n",
    "    # Print formatted summary by category\n",
    "    print(f\"Transition positions: {summary['Transition']}\")\n",
    "    print(f\"Reflection positions: {summary['Reflection']}\")\n",
    "    print(f\"Execution positions: {summary['Execution']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4e439e-ac9e-412e-9a6d-a398a0bf7ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import easysteer.hidden_states as hs\n",
    "from easysteer.steer import StatisticalControlVector\n",
    "import numpy as np\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 1. 初始化 LLM\n",
    "#-------------------------------------------------\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    task=\"reward\",  # reward 模式才能拿 hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 2. 初始化累加器 (按类别和层数)\n",
    "#-------------------------------------------------\n",
    "categories = [\"Transition\", \"Reflection\", \"Execution\"]\n",
    "sums = {cat: {} for cat in categories}\n",
    "counts = {cat: {} for cat in categories}\n",
    "num_layers = None\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 3. 流式处理每个样本\n",
    "#    - 不再一次性保存 all_hidden_states\n",
    "#    - 逐个样本提取 hidden states\n",
    "#    - 在线更新均值\n",
    "#-------------------------------------------------\n",
    "for sample_idx, qa_results in enumerate(all_classifications):\n",
    "    # 获取当前样本 hidden states\n",
    "    hidden_states, outputs = hs.get_all_hidden_states(llm, [qa_pairs[sample_idx]])\n",
    "    # 确定层数（只需第一次）\n",
    "    if num_layers is None:\n",
    "        num_layers = len(hidden_states[0])\n",
    "\n",
    "    # 遍历当前样本中标注的类别位置\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        pos = result[\"position_in_tokens\"]\n",
    "\n",
    "        for layer_idx in range(num_layers):\n",
    "            # 提取该 token 的 hidden state\n",
    "            try:\n",
    "                token_hidden = hidden_states[0][layer_idx][pos].cpu().float().numpy()\n",
    "    \n",
    "                # 初始化累加器\n",
    "                if layer_idx not in sums[category]:\n",
    "                    sums[category][layer_idx] = np.zeros_like(token_hidden)\n",
    "                    counts[category][layer_idx] = 0\n",
    "    \n",
    "                # 累加\n",
    "                sums[category][layer_idx] += token_hidden\n",
    "                counts[category][layer_idx] += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 4. 计算各类别平均向量\n",
    "#-------------------------------------------------\n",
    "average_vectors = {\n",
    "    cat: {\n",
    "        layer: sums[cat][layer] / counts[cat][layer]\n",
    "        for layer in sums[cat] if counts[cat][layer] > 0\n",
    "    }\n",
    "    for cat in sums\n",
    "}\n",
    "\n",
    "#-------------------------------------------------\n",
    "# 5. 导出 GGUF 文件\n",
    "#-------------------------------------------------\n",
    "try:\n",
    "    model_type_str = llm.config.model_type\n",
    "except (AttributeError, NameError):\n",
    "    print(\"Warning: Could not determine model_type from `llm` object. Using a placeholder.\")\n",
    "    model_type_str = \"qwen2\"\n",
    "\n",
    "for category, directions in average_vectors.items():\n",
    "    if not directions:\n",
    "        print(f\"Skipping '{category}' (no data)\")\n",
    "        continue\n",
    "\n",
    "    metadata = {\n",
    "        \"source\": \"Averaged from classified tokens (streaming)\",\n",
    "        \"num_vectors_averaged\": sum(counts[category].values())\n",
    "    }\n",
    "\n",
    "    control_vector = StatisticalControlVector(\n",
    "        model_type=model_type_str,\n",
    "        method=\"Average\",\n",
    "        directions=directions,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    out_path = f\"{category.lower()}_avg_vector.gguf\"\n",
    "    control_vector.export_gguf(out_path)\n",
    "    print(f\"Exported {out_path} with {metadata['num_vectors_averaged']} vectors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447c54c7-5cf4-45f0-8984-89deeb449fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hidden states module to extract model activations\n",
    "import easysteer.hidden_states as hs\n",
    "\n",
    "# Create a new LLM instance in reward mode\n",
    "# Note: This allows us to extract hidden states rather than generating text\n",
    "llm = LLM(\n",
    "    model=\"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\",\n",
    "    task=\"reward\",  # Use reward task to get hidden states\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n",
    "# Extract hidden states for all tokens in the QA pairs\n",
    "all_hidden_states, outputs = hs.get_all_hidden_states(llm, qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3173d-67e6-4c93-8fc4-11f6ec548955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from easysteer.steer import StatisticalControlVector\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Collect all relevant hidden states by category\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize a dictionary to collect all hidden states by category and layer\n",
    "collected_states = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Get the number of layers in the model\n",
    "num_layers = len(all_hidden_states[0])\n",
    "\n",
    "# Process each sample's classification results to collect hidden states\n",
    "for sample_idx, qa_results in enumerate(all_classifications):\n",
    "    for result in qa_results:\n",
    "        category = result[\"category\"]\n",
    "        position = result[\"position_in_tokens\"]\n",
    "\n",
    "        # For each layer, collect hidden states for tokens of this category\n",
    "        for layer_idx in range(num_layers):\n",
    "            # Initialize empty list for this layer if not already present\n",
    "            if layer_idx not in collected_states[category]:\n",
    "                collected_states[category][layer_idx] = []\n",
    "            \n",
    "            # Extract hidden state from the model output\n",
    "            token_hidden = all_hidden_states[sample_idx][layer_idx][position]\n",
    "            \n",
    "            # Convert to numpy for easier processing\n",
    "            token_hidden = token_hidden.cpu().float().numpy()\n",
    "            \n",
    "            # Store the hidden state\n",
    "            collected_states[category][layer_idx].append(token_hidden)\n",
    "\n",
    "\n",
    "# Step 2: Calculate the average hidden state for each category at each layer\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Initialize result dictionaries\n",
    "average_vectors = {\n",
    "    \"Transition\": {},\n",
    "    \"Reflection\": {},\n",
    "    \"Execution\": {}\n",
    "}\n",
    "\n",
    "# Track vector counts for metadata\n",
    "vector_counts = {} \n",
    "\n",
    "# Process each category\n",
    "for category, layer_data in collected_states.items():\n",
    "    # Skip empty categories\n",
    "    if not layer_data:\n",
    "        print(f\"Warning: No vectors found for category '{category}'. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Record how many vectors we're averaging for this category\n",
    "    vector_counts[category] = len(layer_data.get(0, []))\n",
    "    print(f\"Calculating average for '{category}' using {vector_counts[category]} vectors.\")\n",
    "\n",
    "    # Calculate average for each layer\n",
    "    for layer_idx, vectors in layer_data.items():\n",
    "        # Calculate mean across all vectors for this layer\n",
    "        mean_vector = np.mean(np.array(vectors), axis=0)\n",
    "        average_vectors[category][layer_idx] = mean_vector\n",
    "\n",
    "\n",
    "# Step 3: Package and export as GGUF files\n",
    "#-------------------------------------------------\n",
    "\n",
    "# Try to get model type or use a placeholder\n",
    "try:\n",
    "    model_type_str = llm.config.model_type\n",
    "except (AttributeError, NameError):\n",
    "    print(\"Warning: Could not determine model_type from `llm` object. Using a placeholder.\")\n",
    "    model_type_str = \"qwen2\"  # Default placeholder - adjust as needed for your model\n",
    "\n",
    "# Create and export control vectors for each category\n",
    "for category, directions in average_vectors.items():\n",
    "    if not directions:\n",
    "        continue  # Skip if no data available\n",
    "    \n",
    "    # Prepare metadata for the vector\n",
    "    metadata = {\n",
    "        \"source\": \"Averaged from classified newline tokens\",\n",
    "        \"num_vectors_averaged\": vector_counts.get(category, 0)\n",
    "    }\n",
    "\n",
    "    # Create the control vector object\n",
    "    control_vector = StatisticalControlVector(\n",
    "        model_type=model_type_str,\n",
    "        method=\"Average\",\n",
    "        directions=directions,\n",
    "        metadata=metadata\n",
    "    )\n",
    "\n",
    "    # Export to GGUF format\n",
    "    control_vector.export_gguf(f\"{category.lower()}_avg_vector.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df27ea5-5baa-4716-af08-34c742cf06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
