{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d1c81fd-4f91-49cf-bc24-88a55d97afc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yequan/miniconda3/envs/easysteer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 17:00:51 [__init__.py:244] Automatically detected platform cuda.\n",
      "INFO 10-15 17:01:00 [config.py:841] This model supports multiple tasks: {'reward', 'classify', 'generate', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 10-15 17:01:00 [config.py:1472] Using max model len 131072\n",
      "WARNING 10-15 17:01:00 [arg_utils.py:1577] The model has a long context length (131072). This may causeOOM during the initial memory profiling phase, or result in low performance due to small KV cache size. Consider setting --max-model-len to a smaller value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-15 17:01:01,051\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 10-15 17:01:01 [cuda.py:102] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\n",
      "INFO 10-15 17:01:01 [llm_engine.py:232] Initializing a V0 LLM engine (v0.1.dev7506+g9004da86c.d20251015) with config: model='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \n",
      "INFO 10-15 17:01:01 [cuda.py:351] Using Flash Attention backend.\n",
      "INFO 10-15 17:01:03 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 10-15 17:01:03 [model_runner.py:1223] Starting to load model deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 10-15 17:01:03 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
      "INFO 10-15 17:01:03 [weight_utils.py:345] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.92it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-15 17:01:04 [default_loader.py:272] Loading weights took 0.87 seconds\n",
      "INFO 10-15 17:01:05 [model_runner.py:1255] Model loading took 3.3461 GiB and 1.443162 seconds\n",
      "INFO 10-15 17:01:06 [worker.py:295] Memory profiling takes 1.34 seconds\n",
      "INFO 10-15 17:01:06 [worker.py:295] the current vLLM instance can use total_gpu_memory (93.00GiB) x gpu_memory_utilization (0.90) = 83.70GiB\n",
      "INFO 10-15 17:01:06 [worker.py:295] model weights take 3.35GiB; non_torch_memory takes 0.15GiB; PyTorch activation peak memory takes 8.10GiB; the rest of the memory reserved for KV Cache is 72.11GiB.\n",
      "INFO 10-15 17:01:06 [executor_base.py:115] # cuda blocks: 168773, # CPU blocks: 9362\n",
      "INFO 10-15 17:01:06 [executor_base.py:120] Maximum concurrency for 131072 tokens per request: 20.60x\n",
      "INFO 10-15 17:01:10 [llm_engine.py:430] init engine (profile, create kv cache, warmup model) took 4.95 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.steer_vectors.request import SteerVectorRequest, VectorConfig\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    ")\n",
    "\n",
    "# Initialize LLM with steering vector capability\n",
    "llm = LLM(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    enable_steer_vector=True,\n",
    "    enforce_eager=True,\n",
    "    tensor_parallel_size=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "432e3c0b-253a-409f-8fe3-edf89bf555d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the suffix for newline tokens in the tokenizer\n",
    "target_suffix = \"ĊĊ\"  # \"\\n\\n\" is tokenized as \"ĊĊ\"\n",
    "\n",
    "# Get complete tokenizer vocabulary\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Find all tokens and their IDs that end with the target suffix\n",
    "# These are the newline tokens we'll apply steering to\n",
    "matching_tokens_ids = [\n",
    "    token_id\n",
    "    for token, token_id in vocab.items()\n",
    "    if isinstance(token, str) and token.endswith(target_suffix)\n",
    "]\n",
    "\n",
    "# Configure steering vector request for SEAL control\n",
    "sv_request = SteerVectorRequest(\n",
    "    # Name and ID for the steering vector\n",
    "    steer_vector_name=\"complex_control\",\n",
    "    steer_vector_id=4,\n",
    "    \n",
    "    # Configure the three steering vectors (execution, reflection, transition)\n",
    "    vector_configs=[\n",
    "        # Execution vector (positive scale to promote execution-like text)\n",
    "        VectorConfig(\n",
    "            path=\"execution_avg_vector.gguf\",\n",
    "            scale=0.5,                            # Positive scale promotes this behavior\n",
    "            target_layers=[20],                   # Apply at layer 20\n",
    "            generate_trigger_tokens=matching_tokens_ids,  # Apply to newline tokens\n",
    "            algorithm=\"direct\",                   # Direct application\n",
    "            normalize=False                       # Do not normalize vectors\n",
    "        ),\n",
    "        \n",
    "        # Reflection vector (negative scale to suppress reflection)\n",
    "        VectorConfig(\n",
    "            path=\"reflection_avg_vector.gguf\",\n",
    "            scale=-0.5,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\",\n",
    "            normalize=False\n",
    "        ),\n",
    "        \n",
    "        # Transition vector (negative scale to suppress transitions)\n",
    "        VectorConfig(\n",
    "            path=\"transition_avg_vector.gguf\",\n",
    "            scale=-0.5,                           # Negative scale suppresses this behavior\n",
    "            target_layers=[20],\n",
    "            generate_trigger_tokens=matching_tokens_ids,\n",
    "            algorithm=\"direct\", \n",
    "            normalize=False\n",
    "        ),\n",
    "    ],\n",
    "    \n",
    "    # Additional parameters\n",
    "    debug=False,                        # Don't output debug info\n",
    "    conflict_resolution=\"sequential\"    # Apply vectors in sequence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e623edf-0aea-4ef3-9859-6215bd35879c",
   "metadata": {},
   "source": [
    "# MATH500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c8079f-369f-4681-b02d-1a5905f41cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems: ['Convert the point $(0,3)$ in rectangular coordinates to polar coordinates.  Enter your answer in the form $(r,\\\\theta),$ where $r > 0$ and $0 \\\\le \\\\theta < 2 \\\\pi.$', 'Define\\n\\\\[p = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^2} \\\\quad \\\\text{and} \\\\quad q = \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{k^3}.\\\\]Find a way to write\\n\\\\[\\\\sum_{j = 1}^\\\\infty \\\\sum_{k = 1}^\\\\infty \\\\frac{1}{(j + k)^3}\\\\]in terms of $p$ and $q.$']\n",
      "Answers: ['\\\\left( 3, \\\\frac{\\\\pi}{2} \\\\right)', 'p - q']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"/home/yequan/Project/R-KV/HuggingFace/data/math.jsonl\"\n",
    "\n",
    "problems = []\n",
    "answers = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        problems.append(item[\"problem\"])\n",
    "        answers.append(item[\"answer\"])\n",
    "\n",
    "# 看看前两个\n",
    "print(\"Problems:\", problems[:2])\n",
    "print(\"Answers:\", answers[:2])\n",
    "\n",
    "\n",
    "examples = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + prompt + \"\\nAssistant: <think>\" for prompt in problems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "974583e4-6e84-492f-b6ed-42d7badaf121",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 5/5 [00:00<00:00, 119.51it/s]\n",
      "Processed prompts: 100%|██████████| 5/5 [02:19<00:00, 27.93s/it, est. speed input: 4.83 toks/s, output: 104.53 toks/s] \n"
     ]
    }
   ],
   "source": [
    "# Generate response with SEAL steering\n",
    "example_answers = llm.generate(\n",
    "    examples[:5], \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=8192,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91af7056-7aac-446a-97d5-0cea4a2acad6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'math_verify'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmath_verify\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse, verify, LatexExtractionConfig, ExprExtractionConfig\n\u001b[1;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m [output\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m example_answers]\n\u001b[1;32m      3\u001b[0m extraction_target \u001b[38;5;241m=\u001b[39m (ExprExtractionConfig(), LatexExtractionConfig())\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'math_verify'"
     ]
    }
   ],
   "source": [
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n",
    "outputs = [output.outputs[0].text for output in example_answers]\n",
    "extraction_target = (ExprExtractionConfig(), LatexExtractionConfig())\n",
    "results = []\n",
    "for i, llm_output in enumerate(outputs):\n",
    "    gold = parse(f\"${answers[i]}$\", extraction_config=extraction_target)\n",
    "    answer = parse(llm_output, extraction_config=extraction_target)\n",
    "    result = verify(gold, answer)\n",
    "    results.append(result)\n",
    "accuracy = sum(results) / len(results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed09dbaa-a6a9-41a4-b128-344908f536e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  3074.668\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "length = 0\n",
    "for i in range(len(outputs)):\n",
    "    length += len(tokenizer.tokenize(outputs[i], add_special_tokens=True))\n",
    "print(\"Length: \", length/len(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e8934e-ee16-4959-9a5a-5c55a7c6a9bb",
   "metadata": {},
   "source": [
    "# GSM8k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20941347-a40d-4597-8aff-2906f6e92280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problems: [\"Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\", 'A robe takes 2 bolts of blue fiber and half that much white fiber.  How many bolts in total does it take?']\n",
      "Answers: ['Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day.\\nShe makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.\\n#### 18', 'It takes 2/2=<<2/2=1>>1 bolt of white fiber\\nSo the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric\\n#### 3']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "file_path = \"/home/yequan/Project/R-KV/HuggingFace/data/gsm8k.jsonl\"\n",
    "\n",
    "problems = []\n",
    "answers = []\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        item = json.loads(line)\n",
    "        problems.append(item[\"question\"])\n",
    "        answers.append(item[\"answer\"])\n",
    "\n",
    "# 看看前两个\n",
    "print(\"Problems:\", problems[:2])\n",
    "print(\"Answers:\", answers[:2])\n",
    "\n",
    "\n",
    "examples = [\"Please reason step by step, and put your final answer within \\\\boxed{}.\\nUser: \" + prompt + \"\\nAssistant: <think>\" for prompt in problems]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81643a92-d71a-48c0-a066-baea4ae29c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_answers = llm.generate(\n",
    "    examples, \n",
    "    SamplingParams(\n",
    "        temperature=0,\n",
    "        max_tokens=8192,\n",
    "        skip_special_tokens=False,\n",
    "    ), \n",
    "    steer_vector_request=sv_request\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b93431b9-836e-4216-81e4-e57be98a9448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8233510235026535\n"
     ]
    }
   ],
   "source": [
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n",
    "outputs = [output.outputs[0].text for output in example_answers]\n",
    "extraction_target = (ExprExtractionConfig(), LatexExtractionConfig())\n",
    "results = []\n",
    "for i, llm_output in enumerate(outputs):\n",
    "    gold = parse(f\"${answers[i]}$\", extraction_config=extraction_target)\n",
    "    answer = parse(llm_output, extraction_config=extraction_target)\n",
    "    result = verify(gold, answer)\n",
    "    results.append(result)\n",
    "accuracy = sum(results) / len(results)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "851c95b1-e050-438f-b8ba-7b0f1908de12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length:  1460.1266110689917\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/zju-46/shenyl/hf/model/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B/\"\n",
    ")\n",
    "length = 0\n",
    "for i in range(len(outputs)):\n",
    "    length += len(tokenizer.tokenize(outputs[i], add_special_tokens=True))\n",
    "print(\"Length: \", length/len(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cde653-c05f-4da5-b021-e5a8281bb3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easysteer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
